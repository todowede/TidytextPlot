---
title: "TidytextPlot_vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{TidytextPlot_vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r setup,include=FALSE}
library(TidytextPlot)
library(tidytext)
library(dplyr)
library(ggplot2)
```
### Introduction
This vignette describes the content and use of the __*TidytextPlot*__ package to transform a dataframe into tidy data structure and plot a graph of change in proportion of given word(s) over time, where time is indicated by turn. A  cleaned version of dialogues for the three main presidential debates in the 2012 US presidential debate is provided for demonstration. The R package consist of two functions; ```createTidyData``` and  ```plotDebateWords```. The ```createTidyData``` fuction, transforms input dataframe into a tidy text data structure, while function ```plotDebateWords```, calls the ```createTidyData``` function, and generates a ggplot2 graphics. The vignette also illustrate the use of ```bind_tf_idf``` function from the ```tidytext``` package to calculate the __*tf-idf indexes*__, which is the product of the __*term frequency (tf)*__ and the __*inverse document frequency (idf)*__. The vignette also presents and describes a R code that test for __*Zipf's law*__ in the debate data.\

### Installation and Usage
Step 1: Install package \
__*install.packages("path/to/TidytextPlot_1.0.0.11.tar.gz", repos = NULL)*__ 

Step 2: Load package \
__*library(TidytextPlot)*__

Step 3: Load required dependencies: ggplot2, dplyr, tidytext

Step 4: Create plot using included debateData \
__*plotDebateWords(c("china","people","military","iran","energy"),debateData)*__

or

__*plotDebateWords(c("china","people","military","iran","energy"))*__ \
(no need to state second argument if you are using the debateData provided with the package)

Optional Step: To only transform input data to tidy text data structure, use ```createTidyData``` function;e.g. \
__*tidyoutput <- createTidyData(debateData)*__

&nbsp;

### The dataset
The provided data "debateData" is a dataframe containing a cleaned version of the three main predidential debates for the 2012 US election. The variables are as follows:\
* person: the speaker. Obama and Romney were the candidates, Crowley, Lehrer,Schieffer were the moderators and “question” indicates questions raised by the public.\
* dialogue: the words spoken by each person.\
* turn: progressive number indicating the turn of talk.\

### R Function Showing Change of Word Frequency Over Time
Function  ```createTidyData``` reads in the data and transform it to a tidytext (Wickham,H 2014) data structure. 

```{r,message=FALSE}
# createTidyData function takes as argument, an input dataframe 
# with three columns, namely: 'person', 'dialogue' and 'turn'.
createTidyData <- function(input_DF) {
#
# Test that the input data is in the right format, and in particular, 
# the columns names are; 'person', 'dialogue' and 'turn'. 
#
  if(!is.data.frame(input_DF) || any(names(input_DF) != c('person', 'dialogue', 'turn'))) 
    {
    stop("input_DF must be a data.frame with columns 'person','dialogue' and 
         'turn' to continue")
  }
#
# The dataframe can now be transformed into a tidy data. This is done by tokenizing 
# the texts under the dialogue column using the unnest_tokens function in the tidytext 
# package. This process creates the tokenized text under the new column "word".
  unnest_tokens(input_DF, word, dialogue) %>%
# Remove "stop words" with the anti_join function in dplyr.
  anti_join(stop_words) %>%
#
# Calculate the proportion of word for each turn using tools from dplyr package
#
  count(turn,word) %>%
  group_by(turn) %>%
  mutate(p=n/sum(n))
#
# End of the function.It returns the turn, the tokenized word, word count and the 
# proportion of words within each turn. 
}
```

&nbsp;

__Function test 1__: Let's test our function on the debateData included in the package

&nbsp;

```{r,message=FALSE}
test1 <- createTidyData(debateData)
```
View the first few lines of the result
```{r,message=FALSE}
test1 
```

&nbsp;

__Function test 2__: We also want to test our function on a fictitious dataset. So, 
let's generate a fictitious dataframe having the required column names.

&nbsp;

```{r,message=FALSE}
fict_df<-data.frame(person=c("John","Todd","Texas","Rama"),
dialogue=c("We gather here", "meet basic needs","people of this nation", 
"focus on implementation"),
turn=c(1,2,3,4),
stringsAsFactors = FALSE)
```
Run the function test
```{r,message=FALSE}
test2 <- createTidyData(fict_df)
```
View the first few lines of the output
```{r,message=FALSE}
test2
```

&nbsp;

Good, the ```createTidyData``` function work well on the provided data and a fictitious data.

&nbsp;

Now to define the  ```plotDebateWords``` function. This function calls the ```createTidyData``` function, and 
produce a ggplot2 graphics.
```{r,message=FALSE}
#
# The plotDebateWords function takes two inputs as argument. The first 
# argument "word" is set to NULL as default - so  that during the function call, the 
# user can specify the word or words required to plot. The second argument is set to 
# the provided debateData as default. The second argument is however "optional". If the 
# debate data included in this package is used, the second argument is not required. 
# However if user is using own data, the second argument is required.
#
plotDebateWords <- function(word=NULL,data=debateData) {
#
# Call "createTidyData" function to transform the input data (second argument of 
# our function) into tidytext data structure.
#
tidyData <-createTidyData(data)
#
# The next two lines check if the first argument is set to null, and if not null, it 
# retrieve from the data, a subset given by the non-null argument. With %in%, we can use 
# as many words as argument as required.
#
  if(!is.null(word)) {
    tidyData<-tidyData[tidyData$word %in% word,]
  }
#
# Here we plot the data using ggplot
#
plot<-tidyData %>%
    ggplot(aes(turn,p,colour=word)) +
    geom_point() +
    geom_smooth() +
    geom_vline(xintercept = c(541,1798), # Line specifying end of first and 
               # second debate.
               linetype="longdash",
               colour="black",
               size=0.5) +
    facet_wrap(~word, scales="free_y") +
    scale_y_continuous(labels=scales::percent_format()) +
    xlab("Debate turn number") +
    ylab("Percentage of words in presidential debate") +
    ggtitle("Change of Word Frequency in the 2012 Presidential Debates Over Time") +
    theme(legend.position = "none") +
  theme(axis.text = element_text(size = 8, color = "black"), 
        axis.title = element_text(size = 8, color = "black"),
        title = element_text(size = 10))
#
# End of function. It generate a ggplot graphic showing change in proportion of 
# given words over time. The words are provided as the first argument.
return(plot)
}
#
```


&nbsp;

__Function test 3__: We can now test the second function using the provided debateData

&nbsp;


```{r,message=FALSE, fig.width=6, fig.height=4,fig.align="center"}
plotDebateWords("people",debateData)
```

&nbsp;

__Function test 4__:As stated however, since we are using the provided data, we do not need to use the second argument. So we can do,

&nbsp;


```{r,message=FALSE, fig.width=6, fig.height=4,fig.align="center"}
plotDebateWords("people")
```

&nbsp;

__Function test 5__:Test with more than one word with provided data.

&nbsp;

```{r,message=FALSE, fig.width=8, fig.height=4,fig.align="center"}
plotDebateWords(c("people","china","military"))
```

&nbsp;

__Function test 6__: We now test the second function on the fictitious data created in page 3. 
Note that we have to state second argument because we are using a different data.

&nbsp;


```{r,message=FALSE, fig.width=8, fig.height=4,fig.align="center"}
plotDebateWords(c("gather","focus","people"),fict_df)
```

&nbsp;

### Discussion on 2012 US presidential debate
Let's look at the change in frequency of the following words ("jobs", "military", "medicare", "energy", "insurance", "china") over the three main debates.
```{r,message=FALSE, fig.width=8, fig.height=5,fig.align="center"}
plotDebateWords(c("jobs", "military", "medicare", "energy", "insurance", "china"))
```

&nbsp;

Just a brief explanation of above graph.

&nbsp;

__*First Debate*__
Based on the above graph, and considering only the six words (jobs, military, medicare, energy, insurance, china) we have selected, we can see that the first debate focused more on health related dialogues. Insurance (health insurance) and medicare (aka Obamacare) occupied the first segment (before the first dash line) of our graph. Dialogues relating to China on the other hand, seems not to be discussed at all. Towards the end of the first debate however, we see brief debate on military issues. \

&nbsp;

__*Second Debate*__
Debate on China related issues occurred in the second debate with very high interest towards the end of the second debate (second dash line). It can also be seen that energy issues were debated at the start of the second debate.

&nbsp;

__*Third Debate*__
Military affairs dominated the third debate while health issues were completely avoided. China was also discussed later in the third debate. All though the three debates however, jobs related dialogues were debated.


### The tf-idf index
The __*tf-idf*__ index computes the frequency of a term adjusted for how rarely it is used. It is calculated as the product of the term frequency (__tf__) and the inverse document frequency (__idf__ ):

$$tf-idf = tf \times idf.$$

* The term frequency (__tf__ ) identifies how frequently a word occurs in a document. 

* The inverse document frequency (__idf__ ) identify the important words by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection of documents. It is defined as:

$$
idf = \log
(\frac{N}{n_t})$$
where $N$ is the total number of documents being assessed and $n_t$ is the number of documents where the term $t$ appears .

&nbsp;

To calculate the __tf-idf__ for the 2012 US presidential debate, we are simply attempting to find the important words (after adjustment for frequency/rarity of use) mentioned by each of the speakers in the debates dialogues. Put in another way, we are looking for words frequently used by a speaker, but not all the speakers. We use the ```bind_tf_idf``` function to compute the tf-idf.

&nbsp;

Before we do that, we need to have our data in a tidy format. Again, we use the ```unnest_tokens``` function in the tidytext package, which takes in a dataframe of the debate data and tokenize the dialogue, splitting each sentence in separate words. The two arguments for the ```unnest_tokens``` function are column names; the output column (word in this case) and the input column (dialogue in this case). We then use dplyr to remove stop words with an anti_join(). stop words are common english words such as "the", "of", "to", etc, which are not useful for our analysis.

Create tidy text data
```{r,message=FALSE}
library(tidytext)
library(dplyr)

words <- debateData %>%
  unnest_tokens(word,dialogue) %>%
  anti_join(stop_words) %>% 
  count(person,word) 
```
This is followed by the ```bind_tf_idf``` function from the ```tidytext``` package, which takes in the tidy data containing 3 columns; First column (person) contains the debate speakers, the second column (word) contains the tokens and the last column (counts) is the no of times each speaker used a particular word, that is "n" in this case.
```{r,message=FALSE}
words <- words %>%
  bind_tf_idf(word,person,n)
```
Lets look at the idf and tf-idf statistics:
```{r,message=FALSE}
words
```
We expect that the inverse document frequency (__idf__) value, and thus the __tf-idf__ value, will be very low for common words; that is words commonly mentioned by all the speakers. In contrast, the __tf-idf__ value will be higher for words that are unique to a speaker and not commonly used by all the speakers. 

&nbsp;

Let us extract and sort list of words with high __tf-idf__.
```{r,message=FALSE}
words %>%
  arrange(desc(tf_idf))
```
and those with low __tf-idf__
```{r,message=FALSE}
words %>%
  arrange(tf_idf)
```
Here, the __tf-idf__ values are zero for these very common words, which indicates that they are words that are commonly used in the debates by all the speakers. The list is quite realatable. Obama was the incumbent president during the 2012 presidential debate while Romney was the governor of Massachusetts from 2003 to 2007. It is thus expected that during the debate they will be commonly referred to with these titles (i.e. President for Obama and Governor for Romney) by all the speakers, including the candidates themselves. The words with high tf-idf values on the other hand are unique words to the particular speaker. For example, on the list are lot of words from the audience. 

&nbsp;

We now create a plot of top 10 words with the highest tf-idf index for each speaker.
```{r,message=FALSE,fig.width=6, fig.height=6,fig.align='center'}
library(ggplot2)
library(dplyr)
words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(person) %>%
  top_n(10) %>%
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = person)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf_idf index") +
  ggtitle("Highest tf_idf Words in the 2012 US Presidential Debate") +
  facet_wrap(~person, ncol = 2, scales = "free") +
  coord_flip() + 
  theme(axis.text = element_text(size = 8, color = "black"), 
        axis.title = element_text(size = 8, color = "black"),
        title = element_text(size = 12))
```

&nbsp;

Looking at the candidates, there does not seem to be a major difference between them. "jobs" related dialogues seem to be one of their most important campaign/debate strategy. It could be that Obama was emphasizing on how much jobs he created during his first term, while Romney also advertising his jobs creation records when he was a governor. The "million" with high tf-idx index under Romney refer to the millions of jobs he created as governor. So basically "jobs" was the center point of the candidate's message. We can also see that Schieffer questions were focused on global politics and security concerns in Asia, with words like pakistan, afghanistan, war and Iran coming up top. 

&nbsp;

### Zipf's law
We now test our debate data for Zipf's law. Again we need to have our debate data in a tidy data structure, but this time, we are not removing the stopwords as they are important to illustrate Zipf's law. As usual, we use the unnest_tokens() to tokenize the dialogue and create a new column (word) containing the tokenized words, and count the number of times each word is used by the speakers. 
```{r,message=FALSE}
words <- debateData %>%  
  unnest_tokens(word, dialogue) %>%  
  count(person,word, sort = TRUE) %>% 
  ungroup()
```
This is followed by group_by() and summarize() to calculate the total words spoken by each speakers during the debate (i.e. person in our case)
```{r,message=FALSE}
total_words <- words %>%
  group_by(person) %>%
  summarize(total = sum(n))
```
Using left_join() in the dplyr package, we join out tidy text data (words) with the total_words data. With this, for every word, we have the 'n' number of times the particular word is used by a speaker and the 'total' words used by that speaker.
```{r,message=FALSE}
words <- left_join(words, total_words)
```
Then, the term frequency (or tf) can be easily calculated as $(\frac{n}{total})$. With our data already sorted in decreasing order of tf, the rank can be obtained as the row number.
```{r,message=FALSE}
freq_by_rank <- words %>%
  group_by(person) %>%
  mutate(rank = row_number(),
         `term frequency` = n/total)
```
The rank column gives the rank of each word within the frequency table.
```{r,message=FALSE}
freq_by_rank
```
A quick glance of the above table shows that very frequently used english words, such as "the", "to", "that", etc have very low rank as predicted by Zipf's law. As stated earlier, Zipf's law is best observed by plotting the data on a log-log graph, with the axes being log (rank order) and log (frequency).
```{r,message=FALSE,fig.width=6, fig.height=3,fig.align='center'}
freq_by_rank %>%
  ggplot(aes(rank, `term frequency`, color = person)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = TRUE) +
  scale_x_log10() +
  scale_y_log10()
```
We can see that the plot is approximately linear on the log-log graph, and the relationship between rank and the term frequency have a negative slope. Although, there are slight deviations at the high rank and low rank range. This is however not unusual and common in most representation of Zipf's law, e.g. (Adamic.2000). 

&nbsp;

Lets now fit a linear regression model. We however need to adjust for the different lengths of dialogues in the debate data, otherwise, long dialogues would weighted higher than shorter ones. We are therefore going to subset the data to rank smaller than 500 and greater than 10. We use the function lm() to fit the linear models. 
```{r,message=FALSE}
rank_subset <- freq_by_rank %>%
filter(rank < 500,rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
```
As explained earlier, to test for Zipf's law, we hope to get a slope close to -1 on the log-log graph, which is what we have here. We can now use the function geom_abline() to add the regression line to our graph using values of intercept and slope from our linear regression model.
```{r,message=FALSE,fig.width=6, fig.height=3,fig.align='center'}
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = person)) +
geom_abline(intercept = -0.8, slope = -1.0, color = "gray50", linetype = 2) +
geom_line(size = 1.1, alpha = 0.8, show.legend = TRUE) +
scale_x_log10() +
scale_y_log10()
```
It can be seen clearly that our data fit the classic version of Zipf's law - the log(freq) vs log(rank) graph is approximately fitted by a straight line of slope -1.  One other analysis we can infer is the similarity of words used by Obama and Romney, especially within the middle section of the rank range, as we can see that the trend of Obama and Romney lines mimic each other, which is distinct from the other speakers (person)


### References
1.Wickham, H., 2014. Tidy data. Journal of Statistical Software, 59(10), pp.1-23.\
2.Clauset, A., Shalizi, C.R. and Newman, M.E., 2009. Power-law distributions in empirical data. SIAM review, 51(4), pp.661-703.\
3.Li, W.; Miramontes, P.; Cocho, G. Fitting Ranked Linguistic Data with Two-Parameter Functions. Entropy 2010, 12, 1743-1764.\
4.Goldstein, M.L., Morris, S.A. and Yen, G.G., 2004. Problems with fitting to the power-law distribution. The European Physical Journal B-Condensed Matter and Complex Systems, 41(2), pp.255-258.\
5.White, E.P., Enquist, B.J. and Green, J.L., 2008. On estimating the exponent of power‐law frequency distributions. Ecology, 89(4), pp.905-912.\
6.Adamic, L.A., 2000. Zipf, power-laws, and pareto-a ranking tutorial. Xerox Palo Alto Research Center, Palo Alto, CA, http://ginger. hpl. hp. com/shl/papers/ranking/ranking.\
